{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "max_length = 2048\n",
    "dataset = load_dataset(\n",
    "            \"InstaDeepAI/genomics-long-range-benchmark\",\n",
    "            task_name='cage_prediction',\n",
    "            sequence_length=max_length,\n",
    "            cache_dir='data/genomic_long_range',\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "\n",
    "train_dataset = dataset['train']\n",
    "val_dataset = dataset['validation']\n",
    "test_dataset = dataset['test']\n",
    "\n",
    "all_seqs=test_dataset['sequence']\n",
    "all_labels=test_dataset['labels']\n",
    "all_Chromosome=test_dataset['chromosome']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyena_path='outputs/2024-04-28/07-05-57-202372/checkpoints/val/pearsonr_cage.ckpt'\n",
    "\n",
    "batch_size=1\n",
    "\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from scipy import stats\n",
    "def pearsonr_cage(outs, y, len_batch=None):\n",
    "    # TODO: generalize, currently for Monash dataset\n",
    "    metrics = []\n",
    "    outs=outs.detach()\n",
    "    # for i in range(50):\n",
    "    #     y_true = y[:, :,i].cpu().numpy()\n",
    "    #     outs_i = outs[:, :,i].cpu().numpy()\n",
    "    \n",
    "    #     r = stats.pearsonr(y_true.flatten(), outs_i.flatten())[0]\n",
    "    #     metrics.append(r)\n",
    "    \n",
    "    for i in range(outs.shape[-1]):\n",
    "        for j in range(outs.shape[0]):\n",
    "            y_true = y[j, :,i].cpu().numpy()\n",
    "            outs_i = outs[j, :,i].cpu().numpy()\n",
    "            \n",
    "        \n",
    "            r=stats.pearsonr(y_true, outs_i)[0]\n",
    "            metrics.append(r)\n",
    "    #output non nan mean of metrics\n",
    "    output=np.nanmean(metrics)\n",
    "\n",
    "    # x_centered = outs - outs.mean(dim = 1, keepdim = True)\n",
    "    # y_centered = y - y.mean(dim = 1, keepdim = True)\n",
    "    # output=F.cosine_similarity(x_centered, y_centered, dim = 1).mean()\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,AutoModel\n",
    "from torch import nn\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "state_dict='/weight/hyenadna/hyenadna-large-1m-seqlen'\n",
    "d_model=256\n",
    "max_length=2048\n",
    "\n",
    "class hyena_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(hyena_model, self).__init__()\n",
    "        self.backbone=AutoModel.from_pretrained(state_dict, trust_remote_code=True).to('cuda')\n",
    "        self.output_transform = nn.Linear(d_model, 218)\n",
    "        self.linear = nn.Linear(2048,1)\n",
    "        # self.final_pointwise = nn.Sequential(\n",
    "        #     Rearrange('b n d -> b d n'),\n",
    "        #     ConvBlock(d_model, d_model*2, 1),\n",
    "        #     Rearrange('b d n -> b n d'),\n",
    "        #     GELU()\n",
    "        # )\n",
    "        \n",
    "        self.activation=nn.Softplus()\n",
    "    \n",
    "    def forward(self,input_ids,mask=None):\n",
    "        hidden_state=self.backbone(input_ids).last_hidden_state\n",
    "        if mask is None:\n",
    "                restrict = lambda x: (\n",
    "                    torch.cumsum(x, dim=-2)\n",
    "                    / torch.arange(\n",
    "                        1, 1 + x.size(-2), device=x.device, dtype=x.dtype\n",
    "                    ).unsqueeze(-1)\n",
    "                )[..., -1:, :]           \n",
    "        else:\n",
    "                # sum masks\n",
    "                mask_sums = torch.sum(mask, dim=-1).squeeze() - 1  # for 0 indexing\n",
    "\n",
    "                # convert mask_sums to dtype int\n",
    "                mask_sums = mask_sums.type(torch.int64)\n",
    "\n",
    "                restrict = lambda x: (\n",
    "                    torch.cumsum(x, dim=-2)\n",
    "                    / torch.arange(\n",
    "                        1, 1 + x.size(-2), device=x.device, dtype=x.dtype\n",
    "                    ).unsqueeze(-1)\n",
    "                )[torch.arange(x.size(0)), mask_sums, :].unsqueeze(1)  # need to keep original shape\n",
    "        \n",
    "        # hidden_state= (torch.cumsum(hidden_state, dim=-2)\n",
    "        #             / torch.arange(\n",
    "        #                 1, 1 + hidden_state.size(-2), device=hidden_state\n",
    "        #                 .device, dtype=hidden_state.dtype\n",
    "        #             ).unsqueeze(-1)\n",
    "        #         )[..., -1:, :]    \n",
    "        hidden_state=restrict(hidden_state)       \n",
    "        hidden_state = self.output_transform(hidden_state)\n",
    "        output = hidden_state.squeeze(1)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "checkpoint=torch.load('/outputs/2024-05-09/04-54-29-787887/checkpoints/val/spearmanr.ckpt')['state_dict']\n",
    "torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n",
    "            checkpoint, \"model.\"\n",
    "        )\n",
    "torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n",
    "            checkpoint, \"decoder.0.\"\n",
    "        )\n",
    "hyena=hyena_model().to('cuda')\n",
    "hyena.load_state_dict(checkpoint,strict=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,AutoModel\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    state_dict='/weight/hyenadna/hyenadna-large-1m-seqlen'\n",
    "    hyena_tokenizer=AutoTokenizer.from_pretrained(state_dict, trust_remote_code=True)\n",
    "    hyena_model=AutoModel.from_pretrained(state_dict, trust_remote_code=True).to('cuda')\n",
    "    full_sequence=[]\n",
    "    checkpoint=torch.load(hyena_path)['state_dict']\n",
    "    torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n",
    "            checkpoint, \"model.backbone.\"\n",
    "        )\n",
    "    torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n",
    "            checkpoint, \"decoder.0.output_transform.\"\n",
    "        )\n",
    "\n",
    "    hyena_decoder = nn.Linear(256,3).to('cuda')\n",
    "    #edit key name in hyena_decoder\n",
    "    \n",
    "    hyena_model.load_state_dict(checkpoint,strict=False)\n",
    "    hyena_decoder.load_state_dict(checkpoint,strict=False)\n",
    "    hyena_model.eval()\n",
    "    hyena_decoder.eval()\n",
    "\n",
    "    target_list=[]\n",
    "    seq_list=[]\n",
    "    for i in range(len(all_seqs)):\n",
    "        sequence_encoded=hyena_tokenizer(all_seqs[i],\n",
    "                            add_special_tokens= False,  # this is what controls adding eos\n",
    "                            padding=\"max_length\",\n",
    "                            max_length=max_length,\n",
    "                            truncation=True,\n",
    "                        )\n",
    "        seq_ids=sequence_encoded['input_ids']\n",
    "        seq_ids = torch.LongTensor(seq_ids)\n",
    "        target = all_labels[i][:1000]\n",
    "        \n",
    "        seqs=torch.reshape(seq_ids,(1,max_length)).to('cuda')\n",
    "        target_list.append(target)\n",
    "        hidden_states=hyena_model(input_ids=seqs).last_hidden_state\n",
    "        hidden_states=hidden_states[..., :1000, :]\n",
    "        out1=hyena_decoder(hidden_states)\n",
    "        out1_hyena=out1.squeeze(1).squeeze(0).cpu().detach().numpy()\n",
    "        \n",
    "        seq_list.append(out1_hyena)\n",
    "        seq_list_numpy=np.array(seq_list)\n",
    "\n",
    "        target_list_numpy=np.array(target_list)\n",
    "        seq_list_tensor=torch.FloatTensor(seq_list_numpy)\n",
    "        target_list_tensor=torch.FloatTensor(target_list_numpy)\n",
    "        #calculate the \n",
    "        if i>=1:\n",
    "            \n",
    "            \n",
    "            if i%1000==0:\n",
    "                print(i)\n",
    "                pearsonr=pr_auc(seq_list_tensor,target_list_tensor)\n",
    "                print(pearsonr)\n",
    "            \n",
    "\n",
    "#plot the bar plot of the pearsonr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,AutoModel\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "max_length=3000\n",
    "with torch.no_grad():\n",
    "    state_dict='/weight/dnabert2'\n",
    "    bert2_tokenizer=AutoTokenizer.from_pretrained(state_dict, trust_remote_code=True)\n",
    "    bert2_model=AutoModel.from_pretrained(state_dict, trust_remote_code=True).to('cuda')\n",
    "    full_sequence=[]\n",
    "    checkpoint=torch.load(bert2_path)['state_dict']\n",
    "    torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n",
    "            checkpoint, \"model.backbone.\"\n",
    "        )\n",
    "    torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n",
    "            checkpoint, \"decoder.0.output_transform.\"\n",
    "        )\n",
    "\n",
    "    bert2_decoder = nn.Linear(768,3).to('cuda')\n",
    "    #edit key name in hyena_decoder\n",
    "    \n",
    "    bert2_model.load_state_dict(checkpoint,strict=False)\n",
    "    bert2_decoder.load_state_dict(checkpoint,strict=False)\n",
    "    bert2_model.eval()\n",
    "    bert2_decoder.eval()\n",
    "\n",
    "    target_list=[]\n",
    "    seq_list=[]\n",
    "    for i in range(len(all_seqs)):\n",
    "\n",
    "        sequence_encoded=bert2_tokenizer(all_seqs[i],\n",
    "                            add_special_tokens= False,  # this is what controls adding eos\n",
    "                            padding=\"max_length\",\n",
    "                            max_length=max_length,\n",
    "                            truncation=True,\n",
    "                        )\n",
    "        seq_ids=sequence_encoded['input_ids']\n",
    "        seq_ids = torch.LongTensor(seq_ids)\n",
    "        target = all_labels[i][:1000]\n",
    "        \n",
    "        seqs=torch.reshape(seq_ids,(1,max_length)).to('cuda')\n",
    "        target_list.append(target)\n",
    "        hidden_states=bert2_model(input_ids=seqs,export_hidden_states=True)[0]\n",
    "        hidden_states=hidden_states[..., :1000, :]\n",
    "        out1=bert2_decoder(hidden_states)\n",
    "        out1_bert2=out1.squeeze(1).squeeze(0).cpu().detach().numpy()\n",
    "        \n",
    "        seq_list.append(out1_bert2)\n",
    "        seq_list_numpy=np.array(seq_list)\n",
    "\n",
    "        target_list_numpy=np.array(target_list)\n",
    "        seq_list_tensor=torch.FloatTensor(seq_list_numpy)\n",
    "        target_list_tensor=torch.FloatTensor(target_list_numpy)\n",
    "        #calculate the \n",
    "        if i>=1:\n",
    "            \n",
    "            if i%1000==0:\n",
    "                print(i)\n",
    "                pearsonr=pr_auc(seq_list_tensor,target_list_tensor)\n",
    "                print(pearsonr)\n",
    "            \n",
    "\n",
    "#plot the bar plot of the pearsonr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,AutoModel\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "max_length=3000\n",
    "with torch.no_grad():\n",
    "    state_dict='/weight/genalm/gena-lm-bigbird-base-t2t'\n",
    "    genalm_tokenizer=AutoTokenizer.from_pretrained(state_dict, trust_remote_code=True)\n",
    "    genalm_model=AutoModel.from_pretrained(state_dict, trust_remote_code=True).to('cuda')\n",
    "    full_sequence=[]\n",
    "    checkpoint=torch.load(genalm_path)['state_dict']\n",
    "    torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n",
    "            checkpoint, \"model.backbone.\"\n",
    "        )\n",
    "    torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n",
    "            checkpoint, \"decoder.0.output_transform.\"\n",
    "        )\n",
    "\n",
    "    genalm_decoder = nn.Linear(768,3).to('cuda')\n",
    "    #edit key name in hyena_decoder\n",
    "    \n",
    "    genalm_model.load_state_dict(checkpoint,strict=False)\n",
    "    genalm_decoder.load_state_dict(checkpoint,strict=False)\n",
    "    genalm_model.eval()\n",
    "    genalm_decoder.eval()\n",
    "\n",
    "    target_list=[]\n",
    "    seq_list=[]\n",
    "    for i in range(len(all_seqs)):\n",
    "\n",
    "        sequence_encoded=genalm_tokenizer(all_seqs[i],\n",
    "                            add_special_tokens= False,  # this is what controls adding eos\n",
    "                            padding=\"max_length\",\n",
    "                            max_length=max_length,\n",
    "                            truncation=True,\n",
    "                        )\n",
    "        seq_ids=sequence_encoded['input_ids']\n",
    "        seq_ids = torch.LongTensor(seq_ids)\n",
    "        target = all_labels[i][:1000]\n",
    "        \n",
    "        seqs=torch.reshape(seq_ids,(1,max_length)).to('cuda')\n",
    "        target_list.append(target)\n",
    "        hidden_states=genalm_model(input_ids=seqs, output_hidden_states=True,).hidden_states[-1]\n",
    "        hidden_states=hidden_states[..., :1000, :]\n",
    "        out1=genalm_decoder(hidden_states)\n",
    "        out1_genalm=out1.squeeze(1).squeeze(0).cpu().detach().numpy()\n",
    "        \n",
    "        seq_list.append(out1_genalm)\n",
    "        seq_list_numpy=np.array(seq_list)\n",
    "\n",
    "        target_list_numpy=np.array(target_list)\n",
    "        seq_list_tensor=torch.FloatTensor(seq_list_numpy)\n",
    "        target_list_tensor=torch.FloatTensor(target_list_numpy)\n",
    "        #calculate the \n",
    "        if i>=1:\n",
    "            \n",
    "            if i%1000==0:\n",
    "                print(i)\n",
    "                pearsonr=pr_auc(seq_list_tensor,target_list_tensor)\n",
    "                print(pearsonr)\n",
    "            \n",
    "\n",
    "#plot the bar plot of the pearsonr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,AutoModel,AutoModelForMaskedLM\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "max_length=3000\n",
    "with torch.no_grad():\n",
    "    state_dict='weight/nt/nucleotide-transformer-v2-500m-multi-species'\n",
    "    nt_tokenizer=AutoTokenizer.from_pretrained(state_dict, trust_remote_code=True)\n",
    "    nt_model=AutoModelForMaskedLM.from_pretrained(state_dict, trust_remote_code=True).to('cuda')\n",
    "    full_sequence=[]\n",
    "    checkpoint=torch.load(NT_path)['state_dict']\n",
    "    torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n",
    "            checkpoint, \"model.backbone.\"\n",
    "        )\n",
    "    torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n",
    "            checkpoint, \"decoder.0.output_transform.\"\n",
    "        )\n",
    "\n",
    "    nt_decoder = nn.Linear(1024,3).to('cuda')\n",
    "    #edit key name in hyena_decoder\n",
    "    \n",
    "    nt_model.load_state_dict(checkpoint,strict=False)\n",
    "    nt_decoder.load_state_dict(checkpoint,strict=False)\n",
    "    nt_model.eval()\n",
    "    nt_decoder.eval()\n",
    "\n",
    "    target_list=[]\n",
    "    seq_list=[]\n",
    "    for i in range(len(all_seqs)):\n",
    "\n",
    "        sequence_encoded=nt_tokenizer(all_seqs[i],\n",
    "                            add_special_tokens= False,  # this is what controls adding eos\n",
    "                            padding=\"max_length\",\n",
    "                            max_length=max_length,\n",
    "                            truncation=True,\n",
    "                        )\n",
    "        seq_ids=sequence_encoded['input_ids']\n",
    "        seq_ids = torch.LongTensor(seq_ids)\n",
    "        target = all_labels[i][:1000]\n",
    "        \n",
    "        seqs=torch.reshape(seq_ids,(1,max_length)).to('cuda')\n",
    "        target_list.append(target)\n",
    "        hidden_states=nt_model(input_ids=seqs,output_hidden_states=True)['hidden_states'][-1]\n",
    "        hidden_states=hidden_states[..., :1000, :]\n",
    "        out1=nt_decoder(hidden_states)\n",
    "        out1_nt=out1.squeeze(1).squeeze(0).cpu().detach().numpy()\n",
    "        \n",
    "        seq_list.append(out1_nt)\n",
    "        seq_list_numpy=np.array(seq_list)\n",
    "\n",
    "        target_list_numpy=np.array(target_list)\n",
    "        seq_list_tensor=torch.FloatTensor(seq_list_numpy)\n",
    "        target_list_tensor=torch.FloatTensor(target_list_numpy)\n",
    "        #calculate the \n",
    "        if i>=1:\n",
    "            \n",
    "            if i%1000==0:\n",
    "                print(i)\n",
    "                pearsonr=pr_auc(seq_list_tensor,target_list_tensor)\n",
    "                print(pearsonr)\n",
    "            \n",
    "\n",
    "#plot the bar plot of the pearsonr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,AutoModel\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "max_length=3000\n",
    "with torch.no_grad():\n",
    "    state_dict='weight/mamba/caduceus-ph_seqlen-131k_d_model-256_n_layer-16'\n",
    "    mamba_tokenizer=AutoTokenizer.from_pretrained(state_dict, trust_remote_code=True)\n",
    "    mamba_model=AutoModel.from_pretrained(state_dict, trust_remote_code=True).to('cuda')\n",
    "    full_sequence=[]\n",
    "    checkpoint=torch.load(mamba_path)['state_dict']\n",
    "    torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n",
    "            checkpoint, \"model.backbone.\"\n",
    "        )\n",
    "    torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n",
    "            checkpoint, \"decoder.0.output_transform.\"\n",
    "        )\n",
    "\n",
    "    mamba_decoder = nn.Linear(256,3).to('cuda')\n",
    "    #edit key name in hyena_decoder\n",
    "    \n",
    "    mamba_model.load_state_dict(checkpoint,strict=False)\n",
    "    mamba_decoder.load_state_dict(checkpoint,strict=False)\n",
    "    mamba_model.eval()\n",
    "    mamba_decoder.eval()\n",
    "\n",
    "    target_list=[]\n",
    "    seq_list=[]\n",
    "    for i in range(len(all_seqs)):\n",
    "\n",
    "        sequence_encoded=mamba_tokenizer(all_seqs[i],\n",
    "                            add_special_tokens= False,  # this is what controls adding eos\n",
    "                            padding=\"max_length\",\n",
    "                            max_length=max_length,\n",
    "                            truncation=True,\n",
    "                        )\n",
    "        seq_ids=sequence_encoded['input_ids']\n",
    "        seq_ids = torch.LongTensor(seq_ids)\n",
    "        target = all_labels[i][:1000]\n",
    "        \n",
    "        seqs=torch.reshape(seq_ids,(1,max_length)).to('cuda')\n",
    "        target_list.append(target)\n",
    "        hidden_states=mamba_model(seqs,output_hidden_states=True).last_hidden_state\n",
    "        hidden_states=hidden_states[..., :1000, :]\n",
    "        out1=mamba_decoder(hidden_states)\n",
    "        out1_mamba=out1.squeeze(1).squeeze(0).cpu().detach().numpy()\n",
    "        \n",
    "        seq_list.append(out1_mamba)\n",
    "        seq_list_numpy=np.array(seq_list)\n",
    "\n",
    "        target_list_numpy=np.array(target_list)\n",
    "        seq_list_tensor=torch.FloatTensor(seq_list_numpy)\n",
    "        target_list_tensor=torch.FloatTensor(target_list_numpy)\n",
    "        #calculate the \n",
    "        if i>=1:\n",
    "            \n",
    "            if i%1000==0:\n",
    "                print(i)\n",
    "                pearsonr=pr_auc(seq_list_tensor,target_list_tensor)\n",
    "                print(pearsonr)\n",
    "            \n",
    "\n",
    "#plot the bar plot of the pearsonr\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
