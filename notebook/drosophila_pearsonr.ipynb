{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "dest_path ='data/drosophila_enhancer_activity'\n",
    "split='test'\n",
    "targets_file = os.path.join(dest_path, 'Sequences_activity_'+split+\".txt\")\n",
    "data = pd.read_csv(targets_file, sep=\"\\t\")[\n",
    "            [\"Dev_log2_enrichment\", \"Hk_log2_enrichment\"]\n",
    "        ]\n",
    "fasta_file= os.path.join(dest_path, 'Sequences_'+split+\".fa\")\n",
    "all_seqs = []\n",
    "all_labels = []\n",
    "\n",
    "with open(fasta_file) as fin:\n",
    "    header = False\n",
    "    for line in fin:\n",
    "        l = line.strip()\n",
    "        if len(l) == 0:  # last line\n",
    "            break\n",
    "        if line.startswith(\">\"):\n",
    "            header = True\n",
    "            continue\n",
    "        else:\n",
    "            assert header  # check fasta format is correct\n",
    "            all_seqs.append(l)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "all_labels=data[[\"Dev_log2_enrichment\", \"Hk_log2_enrichment\"]].values.astype(\"float32\").tolist()\n",
    "        \n",
    "assert len(all_seqs) == len(\n",
    "            all_labels\n",
    "        ), \"Number of targets does not match number of sequences\"\n",
    "batch_size=128\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "restrict = lambda x: (\n",
    "                    torch.cumsum(x, dim=-2)\n",
    "                    / torch.arange(\n",
    "                        1, 1 + x.size(-2), device=x.device, dtype=x.dtype\n",
    "                    ).unsqueeze(-1)\n",
    "                )[..., -1:, :]   \n",
    "from scipy.stats import pearsonr\n",
    "from scipy import stats\n",
    "def pearsonr_1(outs, y, len_batch=None):\n",
    "    # TODO: generalize, currently for Monash dataset\n",
    "    metrics = {}\n",
    "    outs=outs.detach()\n",
    "    for i, label in enumerate(['dev', 'hk']):\n",
    "        y_true = y[:, i].cpu().numpy()\n",
    "        p = outs[:, i].cpu().numpy()\n",
    "        r = stats.pearsonr(y_true, p)[0]\n",
    "        metrics[f'pearsonr_{label}'] = r\n",
    "        metrics[f'pearsonr2_{label}'] = r ** 2\n",
    "    metrics['pearsonr'] = (metrics['pearsonr_dev'] + metrics['pearsonr_hk']) / 2\n",
    "    return metrics\n",
    "import torch.nn.functional as F\n",
    "def mse(outs, y, len_batch=None):\n",
    "    # assert outs.shape[:-1] == y.shape and outs.shape[-1] == 1\n",
    "    # outs = outs.squeeze(-1)\n",
    "    # if len(y.shape) < len(outs.shape):\n",
    "    #     assert outs.shape[-1] == 1\n",
    "    #     outs = outs.squeeze(-1)\n",
    "    if len_batch is None:\n",
    "        # return F.mse_loss(outs, y)\n",
    "        loss = (\n",
    "                            (\n",
    "                                outs[~torch.isnan(y)]\n",
    "                                - y[~torch.isnan(y)]\n",
    "                            )\n",
    "                            ** 2\n",
    "                        ).mean()\n",
    "        #check if y include nan\n",
    "        if torch.isnan(outs).any():\n",
    "            nan_indices_outs=torch.nonzero(torch.isnan(outs), as_tuple=False)\n",
    "            print(nan_indices_outs)\n",
    "        \n",
    "        if torch.isnan(y).any():\n",
    "            nan_indices=torch.nonzero(torch.isnan(y), as_tuple=False)\n",
    "            print(nan_indices)\n",
    "        return loss\n",
    "    else:\n",
    "        # Computes the loss of the first `lens` items in the batches\n",
    "        # TODO document the use case of this\n",
    "        mask = torch.zeros_like(outs, dtype=torch.bool)\n",
    "        for i, l in enumerate(len_batch):\n",
    "            mask[i, :l, :] = 1\n",
    "        outs_masked = torch.masked_select(outs, mask)\n",
    "        y_masked = torch.masked_select(y, mask)\n",
    "        return F.mse_loss(outs_masked, y_masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,AutoModel\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "max_length=128\n",
    "\n",
    "with torch.no_grad():\n",
    "    state_dict='/weight/hyenadna/hyenadna-large-1m-seqlen'\n",
    "    hyena_tokenizer=AutoTokenizer.from_pretrained(state_dict, trust_remote_code=True)\n",
    "    hyena_model=AutoModel.from_pretrained(state_dict, trust_remote_code=True).to('cuda')\n",
    "    full_sequence=[]\n",
    "    checkpoint=torch.load('/outputs/2024-05-04/13-01-30-043019/checkpoints/val/pearsonr.ckpt')['state_dict']\n",
    "    torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n",
    "            checkpoint, \"model.backbone.\"\n",
    "        )\n",
    "    torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n",
    "            checkpoint, \"decoder.0.output_transform.\"\n",
    "        )\n",
    "\n",
    "    hyena_decoder = nn.Linear(256,2).to('cuda')\n",
    "    #edit key name in hyena_decoder\n",
    "    \n",
    "    hyena_model.load_state_dict(checkpoint,strict=False)\n",
    "    hyena_decoder.load_state_dict(checkpoint,strict=False)\n",
    "    hyena_model.eval()\n",
    "    hyena_decoder.eval()\n",
    "\n",
    "    target_list=[]\n",
    "    seq_list=[]\n",
    "    for i in range(len(all_seqs)):\n",
    "        sequence_encoded=hyena_tokenizer(all_seqs[i],\n",
    "                            add_special_tokens= False,  # this is what controls adding eos\n",
    "                            padding=\"max_length\",\n",
    "                            max_length=max_length,\n",
    "                            truncation=True,\n",
    "                        )\n",
    "        seq_ids=sequence_encoded['input_ids']\n",
    "        seq_ids = torch.LongTensor(seq_ids)\n",
    "        target = all_labels[i]\n",
    "        \n",
    "        seqs=torch.reshape(seq_ids,(1,max_length)).to('cuda')\n",
    "        target_list.append(target)\n",
    "        hidden_states=hyena_model(input_ids=seqs).last_hidden_state\n",
    "        hidden_states=restrict(hidden_states)\n",
    "        out1=hyena_decoder(hidden_states)\n",
    "        out1_hyena=out1.squeeze(1).squeeze(0).cpu().detach().numpy()\n",
    "        \n",
    "        seq_list.append(out1_hyena)\n",
    "        seq_list_tensor=torch.FloatTensor(seq_list)\n",
    "        target_list_tensor=torch.FloatTensor(target_list)\n",
    "        #calculate the \n",
    "        if i>=1:\n",
    "            \n",
    "            \n",
    "            if i%1000==0:\n",
    "                print(i)\n",
    "                pearsonr=pearsonr_1(seq_list_tensor,target_list_tensor)\n",
    "                print(pearsonr)\n",
    "            \n",
    "\n",
    "#plot the bar plot of the pearsonr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,AutoModel\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def group_by_kmer(seq: str, kmer: int) -> str:\n",
    "        return \" \".join(seq[i : i + kmer] for i in range(0, len(seq), kmer)).upper()\n",
    "max_length=128\n",
    "with torch.no_grad():\n",
    "    state_dict='/weight/dnabert/dnabert3/3-new-12w-0'\n",
    "    bert_tokenizer=AutoTokenizer.from_pretrained(state_dict, trust_remote_code=True)\n",
    "    bert_model=AutoModel.from_pretrained(state_dict, trust_remote_code=True).to('cuda')\n",
    "    full_sequence=[]\n",
    "    checkpoint=torch.load('/outputs/2024-03-29/04-45-15-121292/checkpoints/val/pearsonr.ckpt')['state_dict']\n",
    "    torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n",
    "            checkpoint, \"model.backbone.\"\n",
    "        )\n",
    "    torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n",
    "            checkpoint, \"decoder.0.output_transform.\"\n",
    "        )\n",
    "\n",
    "    bert_decoder = nn.Linear(768,2).to('cuda')\n",
    "    #edit key name in hyena_decoder\n",
    "    \n",
    "    bert_model.load_state_dict(checkpoint,strict=False)\n",
    "    bert_decoder.load_state_dict(checkpoint,strict=False)\n",
    "    bert_model.eval()\n",
    "    bert_decoder.eval()\n",
    "\n",
    "    target_list=[]\n",
    "    seq_list=[]\n",
    "    for i in range(len(all_seqs)):\n",
    "        all_seqs_group = group_by_kmer(all_seqs[i],kmer=3)\n",
    "        sequence_encoded=bert_tokenizer(all_seqs_group,\n",
    "                            add_special_tokens= False,  # this is what controls adding eos\n",
    "                            padding=\"max_length\",\n",
    "                            max_length=max_length,\n",
    "                            truncation=True,\n",
    "                        )\n",
    "        seq_ids=sequence_encoded['input_ids']\n",
    "        seq_ids = torch.LongTensor(seq_ids)\n",
    "        target = all_labels[i]\n",
    "        \n",
    "        seqs=torch.reshape(seq_ids,(1,max_length)).to('cuda')\n",
    "        target_list.append(target)\n",
    "        hidden_states=bert_model(input_ids=seqs).last_hidden_state\n",
    "        hidden_states=restrict(hidden_states)\n",
    "        out1=bert_decoder(hidden_states)\n",
    "        out1_bert=out1.squeeze(1).squeeze(0).cpu().detach().numpy()\n",
    "        \n",
    "        seq_list.append(out1_bert)\n",
    "        seq_list_tensor=torch.FloatTensor(seq_list)\n",
    "        target_list_tensor=torch.FloatTensor(target_list)\n",
    "        #calculate the \n",
    "        if i>=1:\n",
    "            \n",
    "            if i%1000==0:\n",
    "                print(i)\n",
    "                pearsonr=pearsonr_1(seq_list_tensor,target_list_tensor)\n",
    "                print(pearsonr)\n",
    "            \n",
    "\n",
    "#plot the bar plot of the pearsonr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,AutoModel\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def group_by_kmer(seq: str, kmer: int) -> str:\n",
    "        return \" \".join(seq[i : i + kmer] for i in range(0, len(seq), kmer)).upper()\n",
    "max_length=128\n",
    "with torch.no_grad():\n",
    "    state_dict='/weight/dnabert2'\n",
    "    bert2_tokenizer=AutoTokenizer.from_pretrained(state_dict, trust_remote_code=True)\n",
    "    bert2_model=AutoModel.from_pretrained(state_dict, trust_remote_code=True).to('cuda')\n",
    "    full_sequence=[]\n",
    "    checkpoint=torch.load('outputs/2024-03-29/08-13-54-405523/checkpoints/val/pearsonr.ckpt')['state_dict']\n",
    "    torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n",
    "            checkpoint, \"model.backbone.\"\n",
    "        )\n",
    "    torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n",
    "            checkpoint, \"decoder.0.output_transform.\"\n",
    "        )\n",
    "\n",
    "    bert2_decoder = nn.Linear(768,2).to('cuda')\n",
    "    #edit key name in hyena_decoder\n",
    "    \n",
    "    bert2_model.load_state_dict(checkpoint,strict=False)\n",
    "    bert2_decoder.load_state_dict(checkpoint,strict=False)\n",
    "    bert2_model.eval()\n",
    "    bert2_decoder.eval()\n",
    "\n",
    "    target_list=[]\n",
    "    seq_list=[]\n",
    "    for i in range(len(all_seqs)):\n",
    "\n",
    "        sequence_encoded=bert2_tokenizer(all_seqs[i],\n",
    "                            add_special_tokens= False,  # this is what controls adding eos\n",
    "                            padding=\"max_length\",\n",
    "                            max_length=max_length,\n",
    "                            truncation=True,\n",
    "                        )\n",
    "        seq_ids=sequence_encoded['input_ids']\n",
    "        seq_ids = torch.LongTensor(seq_ids)\n",
    "        target = all_labels[i]\n",
    "        \n",
    "        seqs=torch.reshape(seq_ids,(1,max_length)).to('cuda')\n",
    "        target_list.append(target)\n",
    "        hidden_states=bert2_model(input_ids=seqs,export_hidden_states=True)[0]\n",
    "        hidden_states=restrict(hidden_states)\n",
    "        out1=bert2_decoder(hidden_states)\n",
    "        out1_bert2=out1.squeeze(1).squeeze(0).cpu().detach().numpy()\n",
    "        \n",
    "        seq_list.append(out1_bert2)\n",
    "        seq_list_tensor=torch.FloatTensor(seq_list)\n",
    "        target_list_tensor=torch.FloatTensor(target_list)\n",
    "        #calculate the \n",
    "        if i>=1:\n",
    "            \n",
    "            if i%1000==0:\n",
    "                print(i)\n",
    "                pearsonr=pearsonr_1(seq_list_tensor,target_list_tensor)\n",
    "                print(pearsonr)\n",
    "            \n",
    "\n",
    "#plot the bar plot of the pearsonr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,AutoModel\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def group_by_kmer(seq: str, kmer: int) -> str:\n",
    "        return \" \".join(seq[i : i + kmer] for i in range(0, len(seq), kmer)).upper()\n",
    "max_length=128\n",
    "with torch.no_grad():\n",
    "    state_dict='/weight/genalm/gena-lm-bert-large-t2t'\n",
    "    genalm_tokenizer=AutoTokenizer.from_pretrained(state_dict, trust_remote_code=True)\n",
    "    genalm_model=AutoModel.from_pretrained(state_dict, trust_remote_code=True).to('cuda')\n",
    "    full_sequence=[]\n",
    "    checkpoint=torch.load('/outputs/2024-03-30/01-49-47-161996/checkpoints/val/pearsonr.ckpt')['state_dict']\n",
    "    torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n",
    "            checkpoint, \"model.backbone.\"\n",
    "        )\n",
    "    torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n",
    "            checkpoint, \"decoder.0.output_transform.\"\n",
    "        )\n",
    "\n",
    "    genalm_decoder = nn.Linear(1024,2).to('cuda')\n",
    "    #edit key name in hyena_decoder\n",
    "    \n",
    "    genalm_model.load_state_dict(checkpoint,strict=False)\n",
    "    genalm_decoder.load_state_dict(checkpoint,strict=False)\n",
    "    genalm_model.eval()\n",
    "    genalm_decoder.eval()\n",
    "\n",
    "    target_list=[]\n",
    "    seq_list=[]\n",
    "    for i in range(len(all_seqs)):\n",
    "\n",
    "        sequence_encoded=genalm_tokenizer(all_seqs[i],\n",
    "                            add_special_tokens= False,  # this is what controls adding eos\n",
    "                            padding=\"max_length\",\n",
    "                            max_length=max_length,\n",
    "                            truncation=True,\n",
    "                        )\n",
    "        seq_ids=sequence_encoded['input_ids']\n",
    "        seq_ids = torch.LongTensor(seq_ids)\n",
    "        target = all_labels[i]\n",
    "        \n",
    "        seqs=torch.reshape(seq_ids,(1,max_length)).to('cuda')\n",
    "        target_list.append(target)\n",
    "        hidden_states=genalm_model(input_ids=seqs, output_hidden_states=True,).hidden_states[-1]\n",
    "        hidden_states=restrict(hidden_states)\n",
    "        out1=genalm_decoder(hidden_states)\n",
    "        out1_genalm=out1.squeeze(1).squeeze(0).cpu().detach().numpy()\n",
    "        \n",
    "        seq_list.append(out1_genalm)\n",
    "        seq_list_tensor=torch.FloatTensor(seq_list)\n",
    "        target_list_tensor=torch.FloatTensor(target_list)\n",
    "        #calculate the \n",
    "        if i>=1:\n",
    "            \n",
    "            if i%1000==0:\n",
    "                print(i)\n",
    "    pearsonr=pearsonr_1(seq_list_tensor,target_list_tensor)\n",
    "    print(pearsonr)\n",
    "            \n",
    "\n",
    "#plot the bar plot of the pearsonr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,AutoModel,AutoModelForMaskedLM\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def group_by_kmer(seq: str, kmer: int) -> str:\n",
    "        return \" \".join(seq[i : i + kmer] for i in range(0, len(seq), kmer)).upper()\n",
    "max_length=128\n",
    "with torch.no_grad():\n",
    "    state_dict='/weight/nt/nucleotide-transformer-v2-500m-multi-species'\n",
    "    nt_tokenizer=AutoTokenizer.from_pretrained(state_dict, trust_remote_code=True)\n",
    "    nt_model=AutoModelForMaskedLM.from_pretrained(state_dict, trust_remote_code=True).to('cuda')\n",
    "    full_sequence=[]\n",
    "    checkpoint=torch.load('/outputs/2024-03-29/10-42-51-953352/checkpoints/val/pearsonr.ckpt')['state_dict']\n",
    "    torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n",
    "            checkpoint, \"model.backbone.\"\n",
    "        )\n",
    "    torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n",
    "            checkpoint, \"decoder.0.output_transform.\"\n",
    "        )\n",
    "\n",
    "    nt_decoder = nn.Linear(1024,2).to('cuda')\n",
    "    #edit key name in hyena_decoder\n",
    "    \n",
    "    nt_model.load_state_dict(checkpoint,strict=False)\n",
    "    nt_decoder.load_state_dict(checkpoint,strict=False)\n",
    "    nt_model.eval()\n",
    "    nt_decoder.eval()\n",
    "\n",
    "    target_list=[]\n",
    "    seq_list=[]\n",
    "    for i in range(len(all_seqs)):\n",
    "\n",
    "        sequence_encoded=nt_tokenizer(all_seqs[i],\n",
    "                            add_special_tokens= False,  # this is what controls adding eos\n",
    "                            padding=\"max_length\",\n",
    "                            max_length=max_length,\n",
    "                            truncation=True,\n",
    "                        )\n",
    "        seq_ids=sequence_encoded['input_ids']\n",
    "        seq_ids = torch.LongTensor(seq_ids)\n",
    "        target = all_labels[i]\n",
    "        \n",
    "        seqs=torch.reshape(seq_ids,(1,max_length)).to('cuda')\n",
    "        target_list.append(target)\n",
    "        hidden_states=nt_model(input_ids=seqs,output_hidden_states=True)['hidden_states'][-1]\n",
    "        hidden_states=restrict(hidden_states)\n",
    "        out1=nt_decoder(hidden_states)\n",
    "        out1_nt=out1.squeeze(1).squeeze(0).cpu().detach().numpy()\n",
    "        \n",
    "        seq_list.append(out1_nt)\n",
    "        seq_list_tensor=torch.FloatTensor(seq_list)\n",
    "        target_list_tensor=torch.FloatTensor(target_list)\n",
    "        #calculate the \n",
    "        if i>=1:\n",
    "            \n",
    "            if i%1000==0:\n",
    "                print(i)\n",
    "    pearsonr=pearsonr_1(seq_list_tensor,target_list_tensor)\n",
    "    print(pearsonr)\n",
    "            \n",
    "\n",
    "#plot the bar plot of the pearsonr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,AutoModel,AutoModelForMaskedLM\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def group_by_kmer(seq: str, kmer: int) -> str:\n",
    "        return \" \".join(seq[i : i + kmer] for i in range(0, len(seq), kmer)).upper()\n",
    "max_length=128\n",
    "with torch.no_grad():\n",
    "    state_dict='/weight/mamba/caduceus-ph_seqlen-131k_d_model-256_n_layer-16'\n",
    "    mamba_tokenizer=AutoTokenizer.from_pretrained(state_dict, trust_remote_code=True)\n",
    "    mamba_model=AutoModel.from_pretrained(state_dict, trust_remote_code=True).to('cuda')\n",
    "    full_sequence=[]\n",
    "    checkpoint=torch.load('/outputs/2024-04-19/14-39-48-177210/checkpoints/val/pearsonr.ckpt')['state_dict']\n",
    "    torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n",
    "            checkpoint, \"model.backbone.\"\n",
    "        )\n",
    "    torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n",
    "            checkpoint, \"decoder.0.output_transform.\"\n",
    "        )\n",
    "\n",
    "    mamba_decoder = nn.Linear(256,2).to('cuda')\n",
    "    #edit key name in hyena_decoder\n",
    "    \n",
    "    mamba_model.load_state_dict(checkpoint,strict=False)\n",
    "    mamba_decoder.load_state_dict(checkpoint,strict=False)\n",
    "    mamba_model.eval()\n",
    "    mamba_decoder.eval()\n",
    "\n",
    "    target_list=[]\n",
    "    seq_list=[]\n",
    "    for i in range(len(all_seqs)):\n",
    "\n",
    "        sequence_encoded=mamba_tokenizer(all_seqs[i],\n",
    "                            add_special_tokens= False,  # this is what controls adding eos\n",
    "                            padding=\"max_length\",\n",
    "                            max_length=max_length,\n",
    "                            truncation=True,\n",
    "                        )\n",
    "        seq_ids=sequence_encoded['input_ids']\n",
    "        seq_ids = torch.LongTensor(seq_ids)\n",
    "        target = all_labels[i]\n",
    "        \n",
    "        seqs=torch.reshape(seq_ids,(1,max_length)).to('cuda')\n",
    "        target_list.append(target)\n",
    "        hidden_states=mamba_model(seqs,output_hidden_states=True).last_hidden_state\n",
    "        hidden_states=restrict(hidden_states)\n",
    "        out1=mamba_decoder(hidden_states)\n",
    "        out1_mamba=out1.squeeze(1).squeeze(0).cpu().detach().numpy()\n",
    "        \n",
    "        seq_list.append(out1_mamba)\n",
    "        seq_list_tensor=torch.FloatTensor(seq_list)\n",
    "        target_list_tensor=torch.FloatTensor(target_list)\n",
    "        #calculate the \n",
    "        if i>=1:\n",
    "            \n",
    "            if i%1000==0:\n",
    "                print(i)\n",
    "    pearsonr=pearsonr_1(seq_list_tensor,target_list_tensor)\n",
    "    print(pearsonr)\n",
    "            \n",
    "\n",
    "#plot the bar plot of the pearsonr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,AutoModel\n",
    "import torch\n",
    "from torch import nn\n",
    "import sys\n",
    "\n",
    "from src.models.sequence.deepSTAR import DeepSTAR\n",
    "max_length=128\n",
    "def genomic_to_one_hot(genomic_sequence):\n",
    "        mapping = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n",
    "        one_hot = np.zeros((len(genomic_sequence), 4))\n",
    "        for i, base in enumerate(genomic_sequence):\n",
    "            if base in mapping:\n",
    "                one_hot[i, mapping[base]] = 1\n",
    "            else:\n",
    "                # 如果碱基不是A、C、G、T或N，可以选择将其编码为全零向量或者平均分配概率\n",
    "                one_hot[i, :] = 0.25  # 或者使用 np.full((5,), 0.2) 平均分配概率\n",
    "        return one_hot\n",
    "with torch.no_grad():\n",
    "    DeepSTAR_model=DeepSTAR(input_size=128,output_size=2)\n",
    "    full_sequence=[]\n",
    "    checkpoint=torch.load('outputs/2024-05-13/09-21-43-779050/checkpoints/val/pearsonr.ckpt')['state_dict']\n",
    "    torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n",
    "            checkpoint, \"model.backbone.\"\n",
    "        )\n",
    "    \n",
    "\n",
    "    #edit key name in hyena_decoder\n",
    "    \n",
    "    DeepSTAR_model.load_state_dict(checkpoint,strict=False)\n",
    "    DeepSTAR_model.to('cuda')\n",
    "    DeepSTAR_model.eval()\n",
    "\n",
    "    target_list=[]\n",
    "    seq_list=[]\n",
    "    for i in range(len(all_seqs)):\n",
    "        seq=genomic_to_one_hot(all_seqs[i])\n",
    "        seq_ids = torch.from_numpy(seq).float()[:max_length,:]\n",
    "        target = all_labels[i]\n",
    "        \n",
    "        seqs=torch.reshape(seq_ids,(1,max_length,-1)).to('cuda')\n",
    "        target_list.append(target)\n",
    "        hidden_states=DeepSTAR_model(seqs)\n",
    "        out1_deepstar=hidden_states.squeeze(1).squeeze(0).cpu().detach().numpy()\n",
    "        \n",
    "        seq_list.append(out1_deepstar)\n",
    "        seq_list_tensor=torch.FloatTensor(seq_list)\n",
    "        target_list_tensor=torch.FloatTensor(target_list)\n",
    "        #calculate the \n",
    "        if i>=1:\n",
    "            \n",
    "            \n",
    "            if i%1000==0:\n",
    "                print(i)\n",
    "                pearsonr=pearsonr_1(seq_list_tensor,target_list_tensor)\n",
    "                print(pearsonr)\n",
    "            \n",
    "\n",
    "#plot the bar plot of the pearsonr\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
