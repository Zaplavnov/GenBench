{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "split = \"test\"\n",
    "dest_path = \"data/splicing_prediction\"\n",
    "all_seqs = np.load(os.path.join(dest_path,  split + \"_seq.npy\"), allow_pickle=True)\n",
    "all_labels = np.load(os.path.join(dest_path,  split + \"_target.npy\"), allow_pickle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyena_path='outputs/2024-04-03/15-12-52-724312/checkpoints/val/pr_auc_mean.ckpt'\n",
    "mamba_path='outputs/2024-04-20/02-29-01-651292/checkpoints/val/pr_auc_mean.ckpt'\n",
    "NT_path='outputs/2024-04-08/01-41-26-952782/checkpoints/val/pr_auc_mean.ckpt'\n",
    "genalm_path='outputs/2024-04-06/11-21-47-532494/checkpoints/val/pr_auc_mean.ckpt'\n",
    "bert2_path='outputs/2024-04-02/18-41-34-140615/checkpoints/val/pr_auc_mean.ckpt'\n",
    "batch_size=1\n",
    "max_length=3000\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "def pr_auc(logits,y):\n",
    "    # compute metrics based on stored labels, predictions, ...\n",
    "    metrics = {}\n",
    "    y, p = y, torch.sigmoid(logits)\n",
    "    #convert three dimension into 2 dimension\n",
    "    y=y.reshape(-1,y.shape[-1]).cpu().numpy()\n",
    "    p=p.reshape(-1,p.shape[-1]).cpu().detach().numpy()\n",
    "    # compute pr-auc for each class independetly\n",
    "    for label in [0, 1, 2]:\n",
    "        y_label = y[:, label]\n",
    "        p_label = p[:, label]\n",
    "        if not np.isnan(p_label).any():\n",
    "            try:\n",
    "                pr_auc = roc_auc_score(y_label, p_label)\n",
    "            except ValueError:\n",
    "                #calculate accurate rate for this label\n",
    "                #convert p_label to 0-1\n",
    "                p_label = (p_label > 0.5).astype(int)\n",
    "                accurate_rate = (y_label == p_label).sum() / len(y_label)\n",
    "                #if accurate rate is 1, set pr_auc to 1\n",
    "                pr_auc = accurate_rate\n",
    "        else:\n",
    "            pr_auc = np.nan\n",
    "        # to be compatible with sklearn 1.1+\n",
    "        metrics[f'pr_auc_{label}'] = pr_auc if not np.isnan(pr_auc) else 0.0\n",
    "    metrics['pr_auc_mean'] = (metrics['pr_auc_1'] + metrics['pr_auc_2']+metrics['pr_auc_0']) / 3\n",
    "    return metrics\n",
    "\n",
    "def accuracy(logits, y):\n",
    "    logits = logits.view(-1, logits.shape[-1])\n",
    "    preds = torch.argmax(logits, dim=-1)\n",
    "    if y.numel() > logits.shape[0]:\n",
    "        # Mixup leads to this case: use argmax class\n",
    "        y = y.argmax(dim=-1)\n",
    "    y = y.view(-1)\n",
    "    return torch.eq(preds, y).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,AutoModel\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    state_dict='weight/hyenadna/hyenadna-large-1m-seqlen'\n",
    "    hyena_tokenizer=AutoTokenizer.from_pretrained(state_dict, trust_remote_code=True)\n",
    "    hyena_model=AutoModel.from_pretrained(state_dict, trust_remote_code=True).to('cuda')\n",
    "    full_sequence=[]\n",
    "    checkpoint=torch.load(hyena_path)['state_dict']\n",
    "    torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n",
    "            checkpoint, \"model.backbone.\"\n",
    "        )\n",
    "    torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n",
    "            checkpoint, \"decoder.0.output_transform.\"\n",
    "        )\n",
    "\n",
    "    hyena_decoder = nn.Linear(256,3).to('cuda')\n",
    "    #edit key name in hyena_decoder\n",
    "    \n",
    "    hyena_model.load_state_dict(checkpoint,strict=False)\n",
    "    hyena_decoder.load_state_dict(checkpoint,strict=False)\n",
    "    hyena_model.eval()\n",
    "    hyena_decoder.eval()\n",
    "\n",
    "    target_list=[]\n",
    "    seq_list=[]\n",
    "    for i in range(len(all_seqs)):\n",
    "        sequence_encoded=hyena_tokenizer(all_seqs[i],\n",
    "                            add_special_tokens= False,  # this is what controls adding eos\n",
    "                            padding=\"max_length\",\n",
    "                            max_length=max_length,\n",
    "                            truncation=True,\n",
    "                        )\n",
    "        seq_ids=sequence_encoded['input_ids']\n",
    "        seq_ids = torch.LongTensor(seq_ids)\n",
    "        target = all_labels[i][:1000]\n",
    "        \n",
    "        seqs=torch.reshape(seq_ids,(1,max_length)).to('cuda')\n",
    "        target_list.append(target)\n",
    "        hidden_states=hyena_model(input_ids=seqs).last_hidden_state\n",
    "        hidden_states=hidden_states[..., :1000, :]\n",
    "        out1=hyena_decoder(hidden_states)\n",
    "        out1_hyena=out1.squeeze(1).squeeze(0).cpu().detach().numpy()\n",
    "        \n",
    "        seq_list.append(out1_hyena)\n",
    "        seq_list_numpy=np.array(seq_list)\n",
    "\n",
    "        target_list_numpy=np.array(target_list)\n",
    "        seq_list_tensor=torch.FloatTensor(seq_list_numpy)\n",
    "        target_list_tensor=torch.FloatTensor(target_list_numpy)\n",
    "        #calculate the \n",
    "        if i>=1:\n",
    "            \n",
    "            \n",
    "            if i%1000==0:\n",
    "                print(i)\n",
    "                pearsonr=pr_auc(seq_list_tensor,target_list_tensor)\n",
    "                print(pearsonr)\n",
    "                accuracy_value=accuracy(seq_list_tensor,target_list_tensor)\n",
    "                print(accuracy_value)\n",
    "            \n",
    "\n",
    "#plot the bar plot of the pearsonr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,AutoModel\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "max_length=3000\n",
    "with torch.no_grad():\n",
    "    state_dict='weight/dnabert2'\n",
    "    bert2_tokenizer=AutoTokenizer.from_pretrained(state_dict, trust_remote_code=True)\n",
    "    bert2_model=AutoModel.from_pretrained(state_dict, trust_remote_code=True).to('cuda')\n",
    "    full_sequence=[]\n",
    "    checkpoint=torch.load(bert2_path)['state_dict']\n",
    "    torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n",
    "            checkpoint, \"model.backbone.\"\n",
    "        )\n",
    "    torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n",
    "            checkpoint, \"decoder.0.output_transform.\"\n",
    "        )\n",
    "\n",
    "    bert2_decoder = nn.Linear(768,3).to('cuda')\n",
    "    #edit key name in hyena_decoder\n",
    "    \n",
    "    bert2_model.load_state_dict(checkpoint,strict=False)\n",
    "    bert2_decoder.load_state_dict(checkpoint,strict=False)\n",
    "    bert2_model.eval()\n",
    "    bert2_decoder.eval()\n",
    "\n",
    "    target_list=[]\n",
    "    seq_list=[]\n",
    "    for i in range(len(all_seqs)):\n",
    "\n",
    "        sequence_encoded=bert2_tokenizer(all_seqs[i],\n",
    "                            add_special_tokens= False,  # this is what controls adding eos\n",
    "                            padding=\"max_length\",\n",
    "                            max_length=max_length,\n",
    "                            truncation=True,\n",
    "                        )\n",
    "        seq_ids=sequence_encoded['input_ids']\n",
    "        seq_ids = torch.LongTensor(seq_ids)\n",
    "        target = all_labels[i][:1000]\n",
    "        \n",
    "        seqs=torch.reshape(seq_ids,(1,max_length)).to('cuda')\n",
    "        target_list.append(target)\n",
    "        hidden_states=bert2_model(input_ids=seqs,export_hidden_states=True)[0]\n",
    "        hidden_states=hidden_states[..., :1000, :]\n",
    "        out1=bert2_decoder(hidden_states)\n",
    "        out1_bert2=out1.squeeze(1).squeeze(0).cpu().detach().numpy()\n",
    "        \n",
    "        seq_list.append(out1_bert2)\n",
    "        seq_list_numpy=np.array(seq_list)\n",
    "\n",
    "        target_list_numpy=np.array(target_list)\n",
    "        seq_list_tensor=torch.FloatTensor(seq_list_numpy)\n",
    "        target_list_tensor=torch.FloatTensor(target_list_numpy)\n",
    "        #calculate the \n",
    "        if i>=1:\n",
    "            \n",
    "            if i%1000==0:\n",
    "                print(i)\n",
    "                pearsonr=pr_auc(seq_list_tensor,target_list_tensor)\n",
    "                print(pearsonr)\n",
    "            \n",
    "\n",
    "#plot the bar plot of the pearsonr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,AutoModel\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "max_length=3000\n",
    "with torch.no_grad():\n",
    "    state_dict='weight/genalm/gena-lm-bigbird-base-t2t'\n",
    "    genalm_tokenizer=AutoTokenizer.from_pretrained(state_dict, trust_remote_code=True)\n",
    "    genalm_model=AutoModel.from_pretrained(state_dict, trust_remote_code=True).to('cuda')\n",
    "    full_sequence=[]\n",
    "    checkpoint=torch.load(genalm_path)['state_dict']\n",
    "    torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n",
    "            checkpoint, \"model.backbone.\"\n",
    "        )\n",
    "    torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n",
    "            checkpoint, \"decoder.0.output_transform.\"\n",
    "        )\n",
    "\n",
    "    genalm_decoder = nn.Linear(768,3).to('cuda')\n",
    "    #edit key name in hyena_decoder\n",
    "    \n",
    "    genalm_model.load_state_dict(checkpoint,strict=False)\n",
    "    genalm_decoder.load_state_dict(checkpoint,strict=False)\n",
    "    genalm_model.eval()\n",
    "    genalm_decoder.eval()\n",
    "\n",
    "    target_list=[]\n",
    "    seq_list=[]\n",
    "    for i in range(len(all_seqs)):\n",
    "\n",
    "        sequence_encoded=genalm_tokenizer(all_seqs[i],\n",
    "                            add_special_tokens= False,  # this is what controls adding eos\n",
    "                            padding=\"max_length\",\n",
    "                            max_length=max_length,\n",
    "                            truncation=True,\n",
    "                        )\n",
    "        seq_ids=sequence_encoded['input_ids']\n",
    "        seq_ids = torch.LongTensor(seq_ids)\n",
    "        target = all_labels[i][:1000]\n",
    "        \n",
    "        seqs=torch.reshape(seq_ids,(1,max_length)).to('cuda')\n",
    "        target_list.append(target)\n",
    "        hidden_states=genalm_model(input_ids=seqs, output_hidden_states=True,).hidden_states[-1]\n",
    "        hidden_states=hidden_states[..., :1000, :]\n",
    "        out1=genalm_decoder(hidden_states)\n",
    "        out1_genalm=out1.squeeze(1).squeeze(0).cpu().detach().numpy()\n",
    "        \n",
    "        seq_list.append(out1_genalm)\n",
    "        seq_list_numpy=np.array(seq_list)\n",
    "\n",
    "        target_list_numpy=np.array(target_list)\n",
    "        seq_list_tensor=torch.FloatTensor(seq_list_numpy)\n",
    "        target_list_tensor=torch.FloatTensor(target_list_numpy)\n",
    "        #calculate the \n",
    "        if i>=1:\n",
    "            \n",
    "            if i%1000==0:\n",
    "                print(i)\n",
    "                pearsonr=pr_auc(seq_list_tensor,target_list_tensor)\n",
    "                print(pearsonr)\n",
    "            \n",
    "\n",
    "#plot the bar plot of the pearsonr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,AutoModel,AutoModelForMaskedLM\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "max_length=3000\n",
    "with torch.no_grad():\n",
    "    state_dict='/weight/nt/nucleotide-transformer-v2-500m-multi-species'\n",
    "    nt_tokenizer=AutoTokenizer.from_pretrained(state_dict, trust_remote_code=True)\n",
    "    nt_model=AutoModelForMaskedLM.from_pretrained(state_dict, trust_remote_code=True).to('cuda')\n",
    "    full_sequence=[]\n",
    "    checkpoint=torch.load(NT_path)['state_dict']\n",
    "    torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n",
    "            checkpoint, \"model.backbone.\"\n",
    "        )\n",
    "    torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n",
    "            checkpoint, \"decoder.0.output_transform.\"\n",
    "        )\n",
    "\n",
    "    nt_decoder = nn.Linear(1024,3).to('cuda')\n",
    "    #edit key name in hyena_decoder\n",
    "    \n",
    "    nt_model.load_state_dict(checkpoint,strict=False)\n",
    "    nt_decoder.load_state_dict(checkpoint,strict=False)\n",
    "    nt_model.eval()\n",
    "    nt_decoder.eval()\n",
    "\n",
    "    target_list=[]\n",
    "    seq_list=[]\n",
    "    for i in range(len(all_seqs)):\n",
    "\n",
    "        sequence_encoded=nt_tokenizer(all_seqs[i],\n",
    "                            add_special_tokens= False,  # this is what controls adding eos\n",
    "                            padding=\"max_length\",\n",
    "                            max_length=max_length,\n",
    "                            truncation=True,\n",
    "                        )\n",
    "        seq_ids=sequence_encoded['input_ids']\n",
    "        seq_ids = torch.LongTensor(seq_ids)\n",
    "        target = all_labels[i][:1000]\n",
    "        \n",
    "        seqs=torch.reshape(seq_ids,(1,max_length)).to('cuda')\n",
    "        target_list.append(target)\n",
    "        hidden_states=nt_model(input_ids=seqs,output_hidden_states=True)['hidden_states'][-1]\n",
    "        hidden_states=hidden_states[..., :1000, :]\n",
    "        out1=nt_decoder(hidden_states)\n",
    "        out1_nt=out1.squeeze(1).squeeze(0).cpu().detach().numpy()\n",
    "        \n",
    "        seq_list.append(out1_nt)\n",
    "        seq_list_numpy=np.array(seq_list)\n",
    "\n",
    "        target_list_numpy=np.array(target_list)\n",
    "        seq_list_tensor=torch.FloatTensor(seq_list_numpy)\n",
    "        target_list_tensor=torch.FloatTensor(target_list_numpy)\n",
    "        #calculate the \n",
    "        if i>=1:\n",
    "            \n",
    "            if i%1000==0:\n",
    "                print(i)\n",
    "                pearsonr=pr_auc(seq_list_tensor,target_list_tensor)\n",
    "                print(pearsonr)\n",
    "            \n",
    "\n",
    "#plot the bar plot of the pearsonr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,AutoModel\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "max_length=3000\n",
    "with torch.no_grad():\n",
    "    state_dict='weight/mamba/caduceus-ph_seqlen-131k_d_model-256_n_layer-16'\n",
    "    mamba_tokenizer=AutoTokenizer.from_pretrained(state_dict, trust_remote_code=True)\n",
    "    mamba_model=AutoModel.from_pretrained(state_dict, trust_remote_code=True).to('cuda')\n",
    "    full_sequence=[]\n",
    "    checkpoint=torch.load(mamba_path)['state_dict']\n",
    "    torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n",
    "            checkpoint, \"model.backbone.\"\n",
    "        )\n",
    "    torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n",
    "            checkpoint, \"decoder.0.output_transform.\"\n",
    "        )\n",
    "\n",
    "    mamba_decoder = nn.Linear(256,3).to('cuda')\n",
    "    #edit key name in hyena_decoder\n",
    "    \n",
    "    mamba_model.load_state_dict(checkpoint,strict=False)\n",
    "    mamba_decoder.load_state_dict(checkpoint,strict=False)\n",
    "    mamba_model.eval()\n",
    "    mamba_decoder.eval()\n",
    "\n",
    "    target_list=[]\n",
    "    seq_list=[]\n",
    "    for i in range(len(all_seqs)):\n",
    "\n",
    "        sequence_encoded=mamba_tokenizer(all_seqs[i],\n",
    "                            add_special_tokens= False,  # this is what controls adding eos\n",
    "                            padding=\"max_length\",\n",
    "                            max_length=max_length,\n",
    "                            truncation=True,\n",
    "                        )\n",
    "        seq_ids=sequence_encoded['input_ids']\n",
    "        seq_ids = torch.LongTensor(seq_ids)\n",
    "        target = all_labels[i][:1000]\n",
    "        \n",
    "        seqs=torch.reshape(seq_ids,(1,max_length)).to('cuda')\n",
    "        target_list.append(target)\n",
    "        hidden_states=mamba_model(seqs,output_hidden_states=True).last_hidden_state\n",
    "        hidden_states=hidden_states[..., :1000, :]\n",
    "        out1=mamba_decoder(hidden_states)\n",
    "        out1_mamba=out1.squeeze(1).squeeze(0).cpu().detach().numpy()\n",
    "        \n",
    "        seq_list.append(out1_mamba)\n",
    "        seq_list_numpy=np.array(seq_list)\n",
    "\n",
    "        target_list_numpy=np.array(target_list)\n",
    "        seq_list_tensor=torch.FloatTensor(seq_list_numpy)\n",
    "        target_list_tensor=torch.FloatTensor(target_list_numpy)\n",
    "        #calculate the \n",
    "        if i>=1:\n",
    "            \n",
    "            if i%1000==0:\n",
    "                print(i)\n",
    "                pearsonr=pr_auc(seq_list_tensor,target_list_tensor)\n",
    "                print(pearsonr)\n",
    "            \n",
    "\n",
    "#plot the bar plot of the pearsonr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,AutoModel\n",
    "import torch\n",
    "from torch import nn\n",
    "import sys\n",
    "\n",
    "from src.models.sequence.SpliceAI import SpliceAI\n",
    "\n",
    "def genomic_to_one_hot(genomic_sequence):\n",
    "        mapping = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n",
    "        one_hot = np.zeros((len(genomic_sequence), 4))\n",
    "        for i, base in enumerate(genomic_sequence):\n",
    "            if base in mapping:\n",
    "                one_hot[i, mapping[base]] = 1\n",
    "            else:\n",
    "                # 如果碱基不是A、C、G、T或N，可以选择将其编码为全零向量或者平均分配概率\n",
    "                one_hot[i, :] = 0.25  # 或者使用 np.full((5,), 0.2) 平均分配概率\n",
    "        return one_hot\n",
    "max_length=3000\n",
    "with torch.no_grad():\n",
    "    splice=SpliceAI(3000,1000)\n",
    "    full_sequence=[]\n",
    "    path='outputs/2024-05-13/05-05-19-201408/checkpoints/val/pr_auc_mean.ckpt'\n",
    "    checkpoint=torch.load(path)['state_dict']\n",
    "    torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n",
    "            checkpoint, \"model.backbone.\"\n",
    "        )\n",
    "    \n",
    "\n",
    "    #edit key name in hyena_decoder\n",
    "    \n",
    "    splice.load_state_dict(checkpoint,strict=False)\n",
    "    splice.to('cuda')\n",
    "    splice.eval()\n",
    "\n",
    "    target_list=[]\n",
    "    seq_list=[]\n",
    "    for i in range(len(all_seqs)):\n",
    "\n",
    "        seq=genomic_to_one_hot(all_seqs[i])\n",
    "        seq_ids = torch.from_numpy(seq).float()[:max_length,:]\n",
    "        target = all_labels[i][:1000]\n",
    "        \n",
    "        seqs=torch.reshape(seq_ids,(1,max_length,-1)).to('cuda')\n",
    "        target_list.append(target)\n",
    "        hidden_states=splice(seqs)\n",
    "        out1_splice=hidden_states.squeeze(1).squeeze(0).cpu().detach().numpy()\n",
    "        \n",
    "        seq_list.append(out1_splice)\n",
    "        seq_list_numpy=np.array(seq_list)\n",
    "\n",
    "        target_list_numpy=np.array(target_list)\n",
    "        seq_list_tensor=torch.FloatTensor(seq_list_numpy)\n",
    "        target_list_tensor=torch.FloatTensor(target_list_numpy)\n",
    "        #calculate the \n",
    "        if i>=1:\n",
    "            \n",
    "            if i%1000==0:\n",
    "                print(i)\n",
    "                pearsonr=pr_auc(seq_list_tensor,target_list_tensor)\n",
    "                print(pearsonr)\n",
    "            \n",
    "\n",
    "#plot the bar plot of the pearsonr\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
